{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test notebook for tunable model. Tests for both classification and regression using Keras and Tensorflow\n",
    "\n",
    "We first import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "sys.path.append('/Users/davidlaredorazo/Documents/University_of_California/Research/Projects')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Reshape, Conv2D, Flatten, MaxPooling2D, LSTM\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "#Use the data handler to manage all the data preprocessing.\n",
    "from ann_framework.data_handlers.data_handler_CMAPSS import CMAPSSDataHandler\n",
    "from ann_framework.data_handlers.data_handler_MNIST import MNISTDataHandler\n",
    "\n",
    "#Import the tunable model classes\n",
    "from ann_framework.tunable_model.tunable_model import SequenceTunableModelRegression, SequenceTunableModelClassification\n",
    "\n",
    "#Import aux functions\n",
    "import ann_framework.aux_functions as aux_functions\n",
    "\n",
    "\n",
    "l2_lambda_regularization = 0.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define keras example models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_test_model_regression(input_shape, output_size=1):\n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(20, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal',\n",
    "                    kernel_regularizer=regularizers.l2(l2_lambda_regularization), name='fc1'))\n",
    "    model.add(Dense(output_size, activation='linear', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.l2(l2_lambda_regularization), name='out'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def keras_test_model_classification(input_shape, output_size=1):\n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(80, input_dim=input_shape, activation='tanh', kernel_initializer='glorot_normal', name='fc1'))\n",
    "    model.add(Dense(output_size, activation='softmax', kernel_initializer='glorot_normal', name='out'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_keras_compiled_model(input_shape, model_type, output_size=1):\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    #Shared parameters for the models\n",
    "    optimizer = Adam(lr=0.001, beta_1=0.5)\n",
    "    \n",
    "    if model_type == \"regression\":\n",
    "        \n",
    "        #Regression model\n",
    "        lossFunction = \"mean_squared_error\"\n",
    "        metrics = [\"mse\"]\n",
    "        model = keras_test_model_regression(input_shape, output_size)\n",
    "    else:\n",
    "    \n",
    "        #Classification model\n",
    "        lossFunction = \"categorical_crossentropy\"\n",
    "        metrics = [\"accuracy\"]\n",
    "        model = keras_test_model_classification(input_shape, output_size)\n",
    "    \n",
    "    model.compile(optimizer = optimizer, loss = lossFunction, metrics = metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tensorflow example models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(input_shape, output_shape):\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=(None,input_shape), name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=None, name=\"y\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def tf_model_classification(X, output_size=1):\n",
    "    \n",
    "    A1 = tf.layers.dense(X, 80, activation=tf.nn.tanh, \n",
    "                         kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False), name=\"fc1\")\n",
    "    logits = tf.layers.dense(A1, output_size, activation=None, \n",
    "                         kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False), name=\"fc2\")\n",
    "    y = tf.nn.softmax(logits)\n",
    "    \n",
    "    return logits, y \n",
    "\n",
    "\n",
    "def tf_model_regression(X, output_size=1):\n",
    "    \n",
    "    A1 = tf.layers.dense(X, 20, activation=tf.nn.relu, \n",
    "                         kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False), \n",
    "                         kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_lambda_regularization), name=\"fc1\")\n",
    "    y = tf.layers.dense(A1, output_size, activation=None, kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                        kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_lambda_regularization), name=\"out\")\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "def get_tf_compiled_model(num_features, model_type, output_size=1):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    X, y = create_placeholders(num_features, output_size)\n",
    "    \n",
    "    if model_type == \"regression\":\n",
    "        \n",
    "        #Regression model\n",
    "        y_pred = tf_model_regression(X, output_size)\n",
    "        cost = tf.losses.mean_squared_error(y_pred, y)\n",
    "    else:\n",
    "    \n",
    "        #Classification model\n",
    "        logits, y_pred = tf_model_classification(X, output_size)\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits))\n",
    "    \n",
    "    reg_cost = tf.losses.get_regularization_loss()\n",
    "    total_cost = cost + reg_cost\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.5).minimize(total_cost)\n",
    "    \n",
    "    return {'X_placeholder':X, 'y_placeholder':y, 'y_pred':y_pred, 'cost':cost, 'total_cost':total_cost, 'optimizer':optimizer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Load CMAPSS data handler\"\"\"\n",
    "\n",
    "#Selected as per CNN paper\n",
    "features = ['T2', 'T24', 'T30', 'T50', 'P2', 'P15', 'P30', 'Nf', 'Nc', 'epr', 'Ps30', 'phi', 'NRf', 'NRc', \n",
    "                     'BPR', 'farB', 'htBleed', 'Nf_dmd', 'PCNfR_dmd', 'W31', 'W32']\n",
    "selected_indices = np.array([2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, 21])\n",
    "selected_features = list(features[i] for i in selected_indices-1)\n",
    "data_folder = '../../NASA_RUL_(CMAPS)/CMAPSSData'\n",
    "\n",
    "window_size = 30\n",
    "window_stride = 1\n",
    "max_rul = 125\n",
    "\n",
    "dHandler_cmaps = CMAPSSDataHandler(data_folder, 1, selected_features, max_rul, window_size, window_stride)\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "\"\"\"Load MNIST data handler\"\"\"\n",
    "dHandler_mnist = MNISTDataHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test TunableModel with regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test TunableModel Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for dataset 1 with window_size of 30, stride of 1 and maxRUL of 125. Cros-Validation ratio 0\n",
      "Loading data from file and computing dataframes\n",
      "Printing shapes\n",
      "\n",
      "Training data (X, y)\n",
      "(17731, 420)\n",
      "(17731, 1)\n",
      "Testing data (X, y)\n",
      "(100, 420)\n",
      "(100, 1)\n",
      "Printing first 5 elements\n",
      "\n",
      "Training data (X, y)\n",
      "[[0.20962199 0.47723785 0.36008634 ... 0.09090909 0.71653543 0.73398665]\n",
      " [0.32302405 0.5314578  0.40992936 ... 0.27272727 0.62992126 0.62647351]\n",
      " [0.39175258 0.43350384 0.43072998 ... 0.27272727 0.69291339 0.86024712]\n",
      " [0.39175258 0.30051151 0.38500785 ... 0.27272727 0.54330709 0.64820338]\n",
      " [0.39862543 0.30204604 0.47036892 ... 0.27272727 0.52755906 0.58940491]]\n",
      "[[125.]\n",
      " [125.]\n",
      " [125.]\n",
      " [125.]\n",
      " [125.]]\n",
      "Testing data (X, y)\n",
      "[[0.17182131 0.44526854 0.25843799 ... 0.36363636 0.52755906 0.65473654]\n",
      " [0.51890034 0.46317136 0.36185243 ... 0.18181818 0.52755906 0.52208493]\n",
      " [0.56701031 0.45677749 0.519427   ... 0.54545455 0.62204724 0.53941202]\n",
      " [0.42611684 0.58414322 0.50215856 ... 0.54545455 0.34645669 0.51683   ]\n",
      " [0.47079038 0.62327366 0.52158556 ... 0.45454545 0.48031496 0.73498083]]\n",
      "[[112.]\n",
      " [ 98.]\n",
      " [ 69.]\n",
      " [ 82.]\n",
      " [ 91.]]\n",
      "training without cv\n",
      "Epoch 1/10\n",
      "17731/17731 [==============================] - 0s 20us/step - loss: 6887.3147 - mean_squared_error: 6879.8173\n",
      "Epoch 2/10\n",
      "17731/17731 [==============================] - 0s 8us/step - loss: 4645.8146 - mean_squared_error: 4638.0527\n",
      "Epoch 3/10\n",
      "17731/17731 [==============================] - 0s 7us/step - loss: 3128.9660 - mean_squared_error: 3119.6493\n",
      "Epoch 4/10\n",
      "17731/17731 [==============================] - 0s 7us/step - loss: 2392.0146 - mean_squared_error: 2380.9912\n",
      "Epoch 5/10\n",
      "17731/17731 [==============================] - 0s 8us/step - loss: 2090.0108 - mean_squared_error: 2077.3360\n",
      "Epoch 6/10\n",
      "17731/17731 [==============================] - 0s 7us/step - loss: 1890.6461 - mean_squared_error: 1875.8104\n",
      "Epoch 7/10\n",
      "17731/17731 [==============================] - 0s 6us/step - loss: 1701.8041 - mean_squared_error: 1684.2960\n",
      "Epoch 8/10\n",
      "17731/17731 [==============================] - 0s 7us/step - loss: 1502.4333 - mean_squared_error: 1481.4337\n",
      "Epoch 9/10\n",
      "17731/17731 [==============================] - 0s 7us/step - loss: 1248.8231 - mean_squared_error: 1221.8904\n",
      "Epoch 10/10\n",
      "17731/17731 [==============================] - 0s 6us/step - loss: 993.1907 - mean_squared_error: 957.5003\n",
      "100/100 [==============================] - 0s 325us/step\n",
      "\n",
      "Test Scores\n",
      "{'loss': 929.1197338867188, 'score_1': 888.7988916015624, 'rhs': array([20.88001865]), 'rmse': 29.868377927165714}\n"
     ]
    }
   ],
   "source": [
    "num_features = len(selected_features)\n",
    "shape = len(selected_features)*window_size\n",
    "\n",
    "#Get keras model\n",
    "model = get_keras_compiled_model(shape, \"regression\", output_size=1)\n",
    "\n",
    "\n",
    "tModel = SequenceTunableModelRegression('ModelRUL_Keras', model, lib_type='keras', data_handler=dHandler_cmaps)\n",
    "\n",
    "tModel.data_handler.data_scaler = None\n",
    "tModel.data_scaler = min_max_scaler\n",
    "\n",
    "tModel.load_data(unroll=True, verbose=1, cross_validation_ratio=0)\n",
    "tModel.print_data()\n",
    "\n",
    "#Train the model\n",
    "tModel.epochs = 10\n",
    "tModel.train_model(verbose=1)\n",
    "tModel.evaluate_model(['rhs', 'rmse'], round=2)\n",
    "print(\"\\nTest Scores\")\n",
    "print(tModel.scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test TunableModel Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for dataset 1 with window_size of 30, stride of 1 and maxRUL of 125. Cros-Validation ratio 0\n",
      "Loading data from file and computing dataframes\n",
      "Printing shapes\n",
      "\n",
      "Training data (X, y)\n",
      "(17731, 420)\n",
      "(17731, 1)\n",
      "Testing data (X, y)\n",
      "(100, 420)\n",
      "(100, 1)\n",
      "Printing first 5 elements\n",
      "\n",
      "Training data (X, y)\n",
      "[[0.20962199 0.47723785 0.36008634 ... 0.09090909 0.71653543 0.73398665]\n",
      " [0.32302405 0.5314578  0.40992936 ... 0.27272727 0.62992126 0.62647351]\n",
      " [0.39175258 0.43350384 0.43072998 ... 0.27272727 0.69291339 0.86024712]\n",
      " [0.39175258 0.30051151 0.38500785 ... 0.27272727 0.54330709 0.64820338]\n",
      " [0.39862543 0.30204604 0.47036892 ... 0.27272727 0.52755906 0.58940491]]\n",
      "[[125.]\n",
      " [125.]\n",
      " [125.]\n",
      " [125.]\n",
      " [125.]]\n",
      "Testing data (X, y)\n",
      "[[0.17182131 0.44526854 0.25843799 ... 0.36363636 0.52755906 0.65473654]\n",
      " [0.51890034 0.46317136 0.36185243 ... 0.18181818 0.52755906 0.52208493]\n",
      " [0.56701031 0.45677749 0.519427   ... 0.54545455 0.62204724 0.53941202]\n",
      " [0.42611684 0.58414322 0.50215856 ... 0.54545455 0.34645669 0.51683   ]\n",
      " [0.47079038 0.62327366 0.52158556 ... 0.45454545 0.48031496 0.73498083]]\n",
      "[[112.]\n",
      " [ 98.]\n",
      " [ 69.]\n",
      " [ 82.]\n",
      " [ 91.]]\n",
      "Epoch: 0001 cost_reg= 7709.101422991 cost= 7705.426422991\n",
      "Epoch: 0002 cost_reg= 6407.890443638 cost= 6404.246986607\n",
      "Epoch: 0003 cost_reg= 4962.375725446 cost= 4957.883886719\n",
      "Epoch: 0004 cost_reg= 3707.519168527 cost= 3701.671463449\n",
      "Epoch: 0005 cost_reg= 2840.497984096 cost= 2833.167068917\n",
      "Epoch: 0006 cost_reg= 2354.648883929 cost= 2346.012611607\n",
      "Epoch: 0007 cost_reg= 2067.816476004 cost= 2058.143240792\n",
      "Epoch: 0008 cost_reg= 1819.560445731 cost= 1808.765359933\n",
      "Epoch: 0009 cost_reg= 1571.597589983 cost= 1559.426803153\n",
      "Epoch: 0010 cost_reg= 1332.640837751 cost= 1318.776708984\n",
      "Epoch:Final cost_reg= 1332.640837751 cost= 1318.776708984\n",
      "\n",
      "Test Scores\n",
      "{'rhs': array([33.79761897]), 'rmse': 34.4127883206229}\n"
     ]
    }
   ],
   "source": [
    "num_features = len(selected_features)\n",
    "shape = len(selected_features)*window_size\n",
    "\n",
    "#Get tensorflow model\n",
    "model = get_tf_compiled_model(shape, \"regression\", output_size=1)\n",
    "\n",
    "\n",
    "tModel = SequenceTunableModelRegression('ModelRUL_tensorflow', model, lib_type='tensorflow', data_handler=dHandler_cmaps)\n",
    "\n",
    "tModel.data_handler.data_scaler = None\n",
    "tModel.data_scaler = min_max_scaler\n",
    "\n",
    "tModel.load_data(unroll=True, verbose=1, cross_validation_ratio=0)\n",
    "tModel.print_data()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "#Train the model\n",
    "tModel.epochs = 10\n",
    "tModel.train_model(tf_session=sess, get_minibatches_function_handle=aux_functions.get_minibatches, verbose=1)\n",
    "tModel.evaluate_model(['rhs', 'rmse'], tf_session=sess, round=2)\n",
    "print(\"\\nTest Scores\")\n",
    "print(tModel.scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test TunableModel with classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test TunableModel Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data. Cross-Validation ratio 0\n",
      "Printing shapes\n",
      "\n",
      "Training data (X, y)\n",
      "(60000, 784)\n",
      "(60000, 10)\n",
      "Testing data (X, y)\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "Printing first 5 elements\n",
      "\n",
      "Training data (X, y)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Testing data (X, y)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "training without cv\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.6547 - acc: 0.8382\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.3073 - acc: 0.9151\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.2507 - acc: 0.9298\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.2151 - acc: 0.9399\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.1885 - acc: 0.9470\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.1678 - acc: 0.9526\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1511 - acc: 0.9577\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.1369 - acc: 0.9620\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1253 - acc: 0.9652\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1148 - acc: 0.9679\n",
      "10000/10000 [==============================] - 0s 34us/step\n",
      "\n",
      "Test Scores\n",
      "{'loss': 0.1295162530094385, 'score_1': 0.9619}\n"
     ]
    }
   ],
   "source": [
    "input_dim = 784\n",
    "\n",
    "#Get keras model\n",
    "model = get_keras_compiled_model(input_dim, \"classification\", output_size=10)\n",
    "\n",
    "\n",
    "tModel = SequenceTunableModelClassification('ModelMNIST_Keras', model, lib_type='keras', data_handler=dHandler_mnist)\n",
    "\n",
    "#tModel.data_handler.data_scaler = None\n",
    "#tModel.data_scaler = min_max_scaler\n",
    "\n",
    "tModel.load_data(unroll=True, verbose=1, cross_validation_ratio=0)\n",
    "tModel.print_data()\n",
    "\n",
    "#Train the model\n",
    "tModel.epochs = 10\n",
    "tModel.train_model(verbose=1)\n",
    "tModel.evaluate_model()\n",
    "print(\"\\nTest Scores\")\n",
    "print(tModel.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data. Cross-Validation ratio 0\n",
      "Printing shapes\n",
      "\n",
      "Training data (X, y)\n",
      "(60000, 784)\n",
      "(60000, 10)\n",
      "Testing data (X, y)\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "Printing first 5 elements\n",
      "\n",
      "Training data (X, y)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Testing data (X, y)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "Epoch: 0001 cost_reg= 0.661378913 cost= 0.661378913\n",
      "Epoch: 0002 cost_reg= 0.307982119 cost= 0.307982119\n",
      "Epoch: 0003 cost_reg= 0.249859576 cost= 0.249859576\n",
      "Epoch: 0004 cost_reg= 0.214853489 cost= 0.214853489\n",
      "Epoch: 0005 cost_reg= 0.188589578 cost= 0.188589578\n",
      "Epoch: 0006 cost_reg= 0.166828428 cost= 0.166828428\n",
      "Epoch: 0007 cost_reg= 0.150683058 cost= 0.150683058\n",
      "Epoch: 0008 cost_reg= 0.136732382 cost= 0.136732382\n",
      "Epoch: 0009 cost_reg= 0.124393019 cost= 0.124393019\n",
      "Epoch: 0010 cost_reg= 0.114521179 cost= 0.114521179\n",
      "Epoch:Final cost_reg= 0.114521179 cost= 0.114521179\n",
      "\n",
      "Test Scores\n",
      "{'accuracy': 0.9638}\n"
     ]
    }
   ],
   "source": [
    "input_dim = 784\n",
    "\n",
    "#Get tensorflow model\n",
    "model = get_tf_compiled_model(input_dim, \"classification\", output_size=10)\n",
    "\n",
    "\n",
    "tModel = SequenceTunableModelClassification('ModelMNIST_Tensorflow', model, lib_type='tensorflow', data_handler=dHandler_mnist)\n",
    "\n",
    "tModel.data_handler.data_scaler = None\n",
    "tModel.data_scaler = min_max_scaler\n",
    "\n",
    "tModel.load_data(unroll=True, verbose=1, cross_validation_ratio=0)\n",
    "tModel.print_data()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "#Train the model\n",
    "tModel.epochs = 10\n",
    "tModel.train_model(tf_session=sess, get_minibatches_function_handle=aux_functions.get_minibatches, verbose=1)\n",
    "tModel.evaluate_model(tf_session=sess, metrics=['accuracy'])\n",
    "print(\"\\nTest Scores\")\n",
    "print(tModel.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
